#########################
# 1) Import & Settings
#########################

import os
import re
import requests
import openai
import streamlit as st
from dotenv import load_dotenv # .env íŒŒì¼ ë¡œë“œ
import json
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

from datetime import datetime
from urllib.parse import urlparse, quote_plus
from bs4 import BeautifulSoup
from newspaper import Article

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# (ì˜µì…˜) googlesearch í™œìš© ì‹œ
# !pip install googlesearch-python
try:
    from googlesearch import search
except ImportError:
    search = None  # ì„¤ì¹˜ ì•ˆ ëœ ê²½ìš° ëŒ€ì²˜ìš©

#########################
# 2) Global Config
#########################

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

# Google Gemini API í‚¤ ì„¤ì •
GEMINI_API_KEY = os.getenv("GOOGLE_GEMINI_API_KEY", "f438158f8d45f67aede74b4a639e23a8365974ce")
genai.configure(api_key=GEMINI_API_KEY)

# ë„ë©”ì¸ë³„ ì‹ ë¢°ë„ ì ìˆ˜ (ì˜ˆì‹œ)
DOMAIN_TRUST_SCORES = {
    "yonhapnews.co.kr": 95,
    "donga.com": 85,
    "chosun.com": 80,
    "seoul.co.kr": 90,
    "kmib.co.kr": 85,
    "news.sbs.co.kr": 75,
    "news.kbs.co.kr": 80,
    "mbc.co.kr": 80,
    "jtbc.joins.com": 75,
    "news.mt.co.kr": 70,
    "news1.kr": 70,
    "newsis.com": 70,
    "newstapa.org": 85,
    "factcheck.kr": 90
}

#########################
# 3) Utility Functions
#########################

def fetch_article(url: str) -> dict:
    """
    ì£¼ì–´ì§„ ê¸°ì‚¬ URLë¡œë¶€í„° ë³¸ë¬¸, ì œëª©, ë°œí–‰ì¼, ë„ë©”ì¸ì„ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜.
    newspaper3kë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ë³¸ë¬¸ ì¶”ì¶œ.
    """
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        
        # ë°œí–‰ì¼ì´ ì—†ëŠ” ê²½ìš° BeautifulSoupìœ¼ë¡œ ì‹œë„
        if not article.publish_date:
            resp = requests.get(url, timeout=10)
            html = resp.text
            date_match = re.search(r'(20\d{2}[-./]\d{1,2}[-./]\d{1,2})', html)
            if date_match:
                try:
                    article.publish_date = datetime.strptime(date_match.group(1), "%Y-%m-%d").date()
                except ValueError:
                    pass

        return {
            "title": article.title or "",
            "text": article.text or "",
            "date": article.publish_date,
            "domain": urlparse(url).netloc
        }
    except Exception as e:
        st.error(f"ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "title": "",
            "text": "",
            "date": None,
            "domain": urlparse(url).netloc
        }

def summarize_article(article_text: str, max_paragraphs: int = 3) -> str:
    """
    ê¸°ì‚¬ ë³¸ë¬¸ì„ GPT-4 ê¸°ë°˜ ëª¨ë¸ë¡œ ìš”ì•½í•˜ì—¬ ë°˜í™˜.
    max_paragraphsë¡œ ë¬¸ë‹¨ ìˆ˜ë¥¼ ì œí•œ.
    """
    if not article_text:
        return "ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    prompt = (
        f"ë‹¤ìŒ ê¸°ì‚¬ë¥¼ {max_paragraphs}ë¬¸ë‹¨ ì´ë‚´ë¡œ í•œêµ­ì–´ ìš”ì•½í•´ì¤˜:\n\n{article_text[:3500]}"
        "\n\n(ë°˜ë“œì‹œ ìš”ì•½ì€ ì§€ì •ëœ ë¬¸ë‹¨ ìˆ˜ ì´ë‚´ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.)"
    )
    
    try:
        # ìµœì‹  OpenAI API í˜•ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        response = client.chat.completions.create(
            model="gpt-4", 
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=500
        )
        summary = response.choices[0].message.content.strip()
        return summary
    except Exception as e:
        st.error(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return "ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def score_domain_trust(domain: str) -> int:
    """
    ê¸°ì‚¬ ì¶œì²˜(ë„ë©”ì¸)ì— ëŒ€í•œ ì‚¬ì „ ì •ì˜ ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë°˜í™˜.
    ì•Œë ¤ì§€ì§€ ì•Šì€ ë„ë©”ì¸ì€ 50ì ìœ¼ë¡œ ì²˜ë¦¬.
    """
    base_domain = domain.lower()
    return DOMAIN_TRUST_SCORES.get(base_domain, 50)

def score_date_recency(publish_date) -> int:
    """
    ê¸°ì‚¬ ë°œí–‰ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ë¥¼, 
    ë°œí–‰ì¼ ì •ë³´ê°€ ì—†ìœ¼ë©´ 0ì ì„ ë¶€ì—¬.
    """
    if not publish_date:
        return 0
    
    # datetime.datetime íƒ€ì…ì„ datetime.date íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    today = datetime.now().date()
    if isinstance(publish_date, datetime):
        publish_date = publish_date.date()
    
    days_diff = (today - publish_date).days
    if days_diff < 0:
        # ë¯¸ë˜ ë‚ ì§œë©´ 0ì 
        return 0
    elif days_diff <= 1:
        return 100  # í•˜ë£¨ ì´ë‚´
    elif days_diff <= 7:
        return 90   # 1ì£¼ì¼ ì´ë‚´
    elif days_diff <= 30:
        return 70   # 1ë‹¬ ì´ë‚´
    elif days_diff <= 365:
        return 50   # 1ë…„ ì´ë‚´
    else:
        return 30   # 1ë…„ ì´ˆê³¼

def extract_claims(text: str, max_claims: int = 5):
    """
    ë³¸ë¬¸ì—ì„œ ì¸ìš©ë¶€í˜¸(â€œ...â€) ì•ˆì— ìˆëŠ” ë¬¸ì¥ì„ ìµœëŒ€ max_claimsê°œê¹Œì§€ ì¶”ì¶œ.
    """
    claims = re.findall(r'â€œ([^"]+)â€', text)
    claims = [c.strip() for c in claims if len(c.strip()) > 0]
    if len(claims) > max_claims:
        claims = claims[:max_claims]
    return claims

def search_web(query: str, num_results: int = 5):
    """
    Google ê²€ìƒ‰ì„ í†µí•´ queryì™€ ê´€ë ¨ëœ URLì„ ìµœëŒ€ num_resultsê°œ ë°˜í™˜.
    (googlesearch ì„¤ì¹˜ í•„ìš”)
    """
    if search is None:
        return []
    results = []
    try:
        for url in search(query, lang='ko', num=num_results, stop=num_results):
            results.append(url)
    except Exception:
        pass
    return results

def fetch_snippet(url: str) -> str:
    """
    ì£¼ì–´ì§„ URLì„ requestsë¡œ ê°€ì ¸ì™€ titleê³¼ meta description(ë˜ëŠ” p íƒœê·¸ ì¼ë¶€)ì„ ì—°ê²°í•´ ë°˜í™˜.
    """
    try:
        resp = requests.get(url, timeout=5)
        if resp.status_code != 200:
            return ""
        soup = BeautifulSoup(resp.text, 'html.parser')
        title = soup.title.string.strip() if soup.title else ""
        meta_desc = soup.find('meta', attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            snippet = meta_desc["content"]
        else:
            # ì²« ë²ˆì§¸ p íƒœê·¸ ì¼ë¶€
            p = soup.find('p')
            snippet = p.get_text(" ", strip=True)[:150] if p else ""
        return f"{title} - {snippet}"
    except:
        return ""

# Google Fact Check API ê´€ë ¨ ì½”ë“œ!!!
# ê¸°ì¡´ get_factcheck_resultsì™€ verify_claim_with_factcheck_api í•¨ìˆ˜ ì œê±°
# def get_factcheck_results(claim: str) -> list:
# def verify_claim_with_factcheck_api(claim: str) -> dict:

def verify_claim_with_gemini(claim: str, evidence_list: list) -> dict:
    """
    Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì¥ì— ëŒ€í•œ íŒ©íŠ¸ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        # ì¦ê±° í…ìŠ¤íŠ¸ ì¤€ë¹„
        evidence_text = "\n".join([f"{i+1}. {ev}" for i, ev in enumerate(evidence_list)])
        
        # í”„ë¡¬í”„íŠ¸ ì‘ì„±
        prompt = f"""ë‹¹ì‹ ì€ ê²€ì¦ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒ©íŠ¸ì²´í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì£¼ì¥ì„ ê²€ì¦í•˜ê³  ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì£¼ì¥: "{claim}"

ì•„ë˜ëŠ” ì´ ì£¼ì¥ê³¼ ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:
{evidence_text}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì£¼ì¥ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:
1. íŒì • - "ì‚¬ì‹¤", "ê±°ì§“", "ë¶€ë¶„ ì‚¬ì‹¤", "ë¶ˆí™•ì‹¤" ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ
2. ì´ìœ  - íŒì • ì´ìœ ë¥¼ ê°„ëµíˆ ì„¤ëª… (2-3 ë¬¸ì¥)

ë‹µë³€ í˜•ì‹: "íŒì •: íŒì • ì´ìœ "
ì˜ˆ: "ì‚¬ì‹¤: ë‹¤ìˆ˜ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì—ì„œ í™•ì¸ëœ ì •ë³´ì…ë‹ˆë‹¤."
"""
        
        # Gemini ëª¨ë¸ ì„¤ì •
        generation_config = {
            "temperature": 0.2,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 300,
        }

        safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        
        # Gemini API í˜¸ì¶œ
        model = genai.GenerativeModel(
            model_name="gemini-1.0-pro",
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        response = model.generate_content(prompt)
        verdict = response.text.strip()
        
        # íŒì • ê²°ê³¼ ë°˜í™˜
        return {
            "verdict": verdict,
            "is_from_gemini": True
        }
    except Exception as e:
        st.error(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "verdict": f"ë¶ˆí™•ì‹¤: Gemini API ì˜¤ë¥˜ - {str(e)}",
            "is_from_gemini": False
        }

# ê¸°ì¡´ get_factcheck_results í•¨ìˆ˜ë¥¼ ëŒ€ì²´í•˜ëŠ” í•¨ìˆ˜
def get_factcheck_with_gemini(claim: str) -> dict:
    """
    ì£¼ì¥ì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ í›„ Gemini APIë¡œ íŒ©íŠ¸ì²´í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
        query = f"{claim} ì‚¬ì‹¤ ì—¬ë¶€"
        result_urls = search_web(query, num_results=5)
        snippets = [fetch_snippet(u) for u in result_urls if u]
        
        if not snippets:
            return {
                "verdict": "ë¶ˆí™•ì‹¤: ê´€ë ¨ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "is_from_gemini": False
            }
        
        # Geminië¡œ íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰
        result = verify_claim_with_gemini(claim, snippets)
        return result
        
    except Exception as e:
        st.error(f"íŒ©íŠ¸ì²´í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "verdict": "ë¶ˆí™•ì‹¤ (ì²˜ë¦¬ ì˜¤ë¥˜)",
            "is_from_gemini": False
        }

# Gemini ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
def score_content_trust(claim_verifications: dict) -> int:
    """
    Geminiê°€ íŒì •í•œ 'ì‚¬ì‹¤/ê±°ì§“/ë¶ˆí™•ì‹¤/ë¶€ë¶„ ì‚¬ì‹¤' ê²°ê³¼ë¥¼ ì¢…í•©í•´ ê¸°ì‚¬ ë‚´ìš© ì‹ ë¢°ë„(0~100) ì‚°ì •.
    - ì‚¬ì‹¤: +1
    - ê±°ì§“: -0.2 ì´ì ì—ì„œ 20ì  ê°ì‚°
    - ë¶ˆí™•ì‹¤: 0.5ë¡œ ì²˜ë¦¬ 
    - ë¶€ë¶„ ì‚¬ì‹¤: 0.7ë¡œ ì²˜ë¦¬
    """
    if not claim_verifications:
        return 50  # ê²€ì¦í•  ì£¼ì¥ ì—†ìŒ â†’ ì¤‘ë¦½ ì ìˆ˜

    results = list(claim_verifications.values())
    # ì‚¬ì‹¤ íŒì •
    fact_count = sum("ì‚¬ì‹¤:" in r or "ì‚¬ì‹¤ " in r or r.startswith("ì‚¬ì‹¤") for r in results)
    # ê±°ì§“ íŒì •
    false_count = sum("ê±°ì§“:" in r or "ê±°ì§“ " in r or r.startswith("ê±°ì§“") for r in results)
    # ë¶ˆí™•ì‹¤ íŒì •
    uncertain_count = sum("ë¶ˆí™•ì‹¤:" in r or "ë¶ˆí™•ì‹¤ " in r or r.startswith("ë¶ˆí™•ì‹¤") for r in results)
    # ë¶€ë¶„ ì‚¬ì‹¤ íŒì •
    partial_count = sum("ë¶€ë¶„ ì‚¬ì‹¤:" in r or "ë¶€ë¶„ ì‚¬ì‹¤ " in r or "ë¶€ë¶„ì‚¬ì‹¤" in r or r.startswith("ë¶€ë¶„ ì‚¬ì‹¤") for r in results)

    total_claims = len(results)

    # ì‚¬ì‹¤ ë¹„ìœ¨
    fact_ratio = fact_count / total_claims
    # ë¶€ë¶„ ì‚¬ì‹¤ ì ìˆ˜ (70%)
    partial_ratio = 0.7 * partial_count / total_claims
    # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
    base_score = (fact_ratio + partial_ratio) * 100.0
    # í˜ë„í‹° ê³„ì‚°
    false_penalty = false_count * 20
    uncertain_penalty = uncertain_count * 10

    final_score = base_score - false_penalty - uncertain_penalty
    if final_score < 0:
        final_score = 0
    if final_score > 100:
        final_score = 100
    return int(final_score)

#########################
# 4) Streamlit App
#########################

def main():
    st.set_page_config(
        page_title="ë‰´ìŠ¤ ê¸°ì‚¬ ì‹ ë¢°ë„ í‰ê°€ê¸°",
        page_icon="ğŸ“°",
        layout="wide"
    )
    
    st.title("ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ ì‹ ë¢°ë„ í‰ê°€ê¸°")
    st.markdown("""
    ì´ ì•±ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì‹ ë¢°ë„ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤:
    - ì¶œì²˜ ì‹ ë¢°ë„ (30%): ë‰´ìŠ¤ ë§¤ì²´ì˜ ì‹ ë¢°ë„
    - ë‚ ì§œ ì‹ ë¢°ë„ (20%): ê¸°ì‚¬ì˜ ìµœì‹ ì„±
    - ë‚´ìš© ì‹ ë¢°ë„ (50%): ê¸°ì‚¬ ë‚´ìš©ì˜ ì‚¬ì‹¤ ì—¬ë¶€
    """)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        url_input = st.text_input("ê¸°ì‚¬ URL ì…ë ¥:", placeholder="https://example.com/news/article")
        
        if st.button("í‰ê°€í•˜ê¸°", type="primary"):
            if not url_input.strip():
                st.error("ìœ íš¨í•œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return
            
            with st.spinner("ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                article_data = fetch_article(url_input)
            
            if not article_data["text"]:
                st.error("ê¸°ì‚¬ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URLì´ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return
            
            # 1) ìš”ì•½
            with st.spinner("ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘..."):
                summary = summarize_article(article_data["text"])

            # 2) ì¶œì²˜ ì‹ ë¢°ë„
            domain_score = score_domain_trust(article_data["domain"])
            # 3) ë‚ ì§œ ì‹ ë¢°ë„
            date_score = score_date_recency(article_data["date"])
            # 4) ì£¼ìš” ì£¼ì¥ ì¶”ì¶œ
            claims = extract_claims(article_data["text"], max_claims=5)

            # 5) ì›¹ ê²€ìƒ‰ & ê²€ì¦
            claim_verifications = {}
            if claims:
                with st.spinner("ì£¼ìš” ì£¼ì¥ë“¤ì„ ê²€ì¦í•˜ëŠ” ì¤‘..."):
                    for c in claims:
                        # Google Geminië¥¼ í™œìš©í•œ íŒ©íŠ¸ì²´í¬
                        result = get_factcheck_with_gemini(c)
                        claim_verifications[c] = result["verdict"]
                        
                        # Geminiì—ì„œ ê²€ì¦ëœ ê²°ê³¼ì¸ ê²½ìš° í‘œì‹œ
                        if result["is_from_gemini"]:
                            st.info(f"ì£¼ì¥ \"{c}\"ì€(ëŠ”) Google Gemini AIë¡œ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # 6) ë‚´ìš© ì‹ ë¢°ë„ ì ìˆ˜
            content_score = score_content_trust(claim_verifications)

            # 7) ì¢…í•© ì ìˆ˜
            overall_score = int(domain_score*0.3 + date_score*0.2 + content_score*0.5)

            # ê²°ê³¼ ì¶œë ¥
            st.subheader("ğŸ“ ê¸°ì‚¬ ìš”ì•½")
            st.write(summary)

            # ì‹ ë¢°ë„ ì ìˆ˜ ì‹œê°í™”
            st.subheader("ğŸ“Š ì‹ ë¢°ë„ í‰ê°€")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ì¶œì²˜ ì‹ ë¢°ë„", f"{domain_score}/100", f"{domain_score-50:+d}")
            with col2:
                st.metric("ë‚ ì§œ ì‹ ë¢°ë„", f"{date_score}/100", f"{date_score-50:+d}")
            with col3:
                st.metric("ë‚´ìš© ì‹ ë¢°ë„", f"{content_score}/100", f"{content_score-50:+d}")
            with col4:
                st.metric("ì¢…í•© ì‹ ë¢°ë„", f"{overall_score}/100", f"{overall_score-50:+d}")
            
            # ì£¼ì¥ë³„ ê²€ì¦ ê²°ê³¼
            if claim_verifications:
                st.subheader("ğŸ” ê²€ì¦ëœ ì£¼ìš” ì£¼ì¥ë“¤")
                for claim, result in claim_verifications.items():
                    st.markdown(f"- **\"{claim}\"** â†’ {result}")

            st.success("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    with col2:
        st.subheader("â„¹ï¸ ì‚¬ìš© ì•ˆë‚´")
        st.markdown("""
        1. ë‰´ìŠ¤ ê¸°ì‚¬ì˜ URLì„ ì…ë ¥ì°½ì— ë¶™ì—¬ë„£ê¸° í•©ë‹ˆë‹¤.
        2. 'í‰ê°€í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•©ë‹ˆë‹¤.
        3. ì ì‹œ í›„ ê¸°ì‚¬ì˜ ìš”ì•½ê³¼ ì‹ ë¢°ë„ í‰ê°€ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.
        
        **ì°¸ê³ **: 
        - í‰ê°€ì—ëŠ” ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.
        - OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        """)

# Streamlitì—ì„œ ì‹¤í–‰
if __name__ == "__main__":
    main()
